"""
Service pour tÃ©lÃ©charger et parser des datasets (CSV, Excel)
UtilisÃ© pour extraire des donnÃ©es quantitatives depuis des URLs
"""

import httpx
import csv
import io
import math
from typing import List, Dict, Optional
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np


class DataService:
    """Service pour tÃ©lÃ©charger et parser des datasets"""
    
    def __init__(self):
        self.supported_formats = ['.csv', '.xlsx', '.xls']
        self.max_file_size = 10 * 1024 * 1024  # 10 MB max
        self.min_rows = 5  # Minimum 5 lignes (ajustÃ© pour statistiques concises)
        self.min_data_density = 0.3  # Minimum 30% de donnÃ©es non-nulles
        
        # ModÃ¨le d'embeddings pour la recherche sÃ©mantique
        print("ğŸ¤– Chargement du modÃ¨le d'embeddings...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        print("âœ… ModÃ¨le d'embeddings chargÃ©")
    
    @staticmethod
    def clean_nan_values(obj):
        """
        Nettoie rÃ©cursivement tous les NaN, Infinity et autres valeurs non-JSON d'un objet
        
        Args:
            obj: L'objet Ã  nettoyer (dict, list, ou valeur simple)
        
        Returns:
            L'objet nettoyÃ© avec tous les NaN remplacÃ©s par None
        """
        if isinstance(obj, dict):
            return {k: DataService.clean_nan_values(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [DataService.clean_nan_values(item) for item in obj]
        elif isinstance(obj, float):
            # VÃ©rifier NaN, Infinity
            if math.isnan(obj) or math.isinf(obj):
                return None
            return obj
        else:
            return obj
    
    def validate_dataset_quality(self, dataset: Dict) -> bool:
        """
        Valide qu'un dataset a suffisamment de lignes et de donnÃ©es non-vides
        
        Args:
            dataset: Le dataset Ã  valider
        
        Returns:
            True si le dataset est valide, False sinon
        """
        try:
            total_rows = dataset.get('total_rows', 0)
            rows = dataset.get('rows', [])
            
            # VÃ©rifier nombre minimum de lignes
            if total_rows < self.min_rows:
                print(f"âš ï¸ Dataset rejetÃ©: seulement {total_rows} lignes (min {self.min_rows})")
                return False
            
            # Calculer la densitÃ© de donnÃ©es (% de valeurs non-nulles)
            if not rows:
                return False
            
            total_cells = 0
            non_null_cells = 0
            
            for row in rows:
                for value in row.values():
                    total_cells += 1
                    if value is not None and value != '' and value != '-':
                        non_null_cells += 1
            
            if total_cells == 0:
                return False
            
            data_density = non_null_cells / total_cells
            
            if data_density < self.min_data_density:
                print(f"âš ï¸ Dataset rejetÃ©: seulement {data_density*100:.1f}% de donnÃ©es (min {self.min_data_density*100}%)")
                return False
            
            print(f"âœ… Dataset validÃ©: {total_rows} lignes, {data_density*100:.1f}% de densitÃ©")
            return True
            
        except Exception as e:
            print(f"âŒ Erreur validation dataset: {e}")
            return False
    
    def extract_keywords_from_dataset(self, dataset: Dict) -> str:
        """
        Extrait les mots-clÃ©s d'un dataset (colonnes + quelques valeurs) pour validation
        
        Args:
            dataset: Le dataset parsÃ©
        
        Returns:
            String contenant colonnes et valeurs pour analyse
        """
        try:
            columns = dataset.get('columns', [])
            preview = dataset.get('preview', [])
            
            # Texte searchable: colonnes + premiÃ¨res valeurs
            keywords_text = f"Colonnes: {', '.join([str(c) for c in columns[:10]])}. "
            
            # Ajouter quelques valeurs
            if preview:
                for i, row in enumerate(preview[:2]):
                    keywords_text += f"Ligne {i+1}: {', '.join([f'{k}={v}' for k, v in list(row.items())[:5]])}. "
            
            return keywords_text
            
        except Exception as e:
            return ""
    
    async def search_all_apis(self, query: str) -> List[Dict]:
        """
        Recherche de datasets via toutes les APIs disponibles
        
        Args:
            query: Termes de recherche
        
        Returns:
            Liste de datasets avec metadata
        """
        all_datasets = []
        
        # 1. data.gouv.fr
        datagouv_results = await self.search_datagouv_api(query)
        all_datasets.extend(datagouv_results)
        
        # 2. INSEE (sÃ©ries temporelles)
        insee_results = await self.search_insee_api(query)
        all_datasets.extend(insee_results)
        
        # DÃ©dupliquer par URL
        seen_urls = set()
        unique_datasets = []
        for ds in all_datasets:
            if ds['url'] not in seen_urls:
                seen_urls.add(ds['url'])
                unique_datasets.append(ds)
        
        print(f"ğŸ“Š Total datasets uniques: {len(unique_datasets)}")
        return unique_datasets[:5]  # Limiter Ã  5 meilleurs
    
    def _calculate_semantic_score(self, item: Dict, query_embedding: np.ndarray) -> float:
        """
        Calcule un score de pertinence sÃ©mantique pour un dataset
        
        Args:
            item: Dataset data.gouv.fr
            query_embedding: Embedding de la requÃªte utilisateur
        
        Returns:
            Score de pertinence (0-100, plus Ã©levÃ© = plus pertinent)
        """
        # Construire le texte du dataset
        title = item.get("title") or ""
        description = item.get("description") or ""
        dataset_text = f"{title}. {description[:300]}"  # Limiter la description
        
        # Encoder le dataset
        dataset_embedding = self.embedding_model.encode([dataset_text])
        
        # SimilaritÃ© cosinus (0 Ã  1)
        semantic_similarity = cosine_similarity(query_embedding.reshape(1, -1), dataset_embedding)[0][0]
        
        # Score de base : similaritÃ© sÃ©mantique (0-70 points)
        score = semantic_similarity * 70
        
        # Bonus 1 : Nombre de ressources CSV/Excel (+5 max)
        resources = item.get("resources", [])
        csv_resources = [r for r in resources if (r.get("format") or "").lower() in ['csv', 'xlsx', 'xls']]
        score += min(len(csv_resources) * 2, 5)
        
        # Bonus 2 : PopularitÃ© (+10 max)
        metrics = item.get("metrics") or {}
        followers = metrics.get("followers", 0) if isinstance(metrics, dict) else 0
        score += min(followers / 10, 10)
        
        # Bonus 3 : QualitÃ© de la description (+5 max)
        desc_length = len(description)
        if desc_length > 200:
            score += 5
        elif desc_length > 100:
            score += 3
        elif desc_length > 50:
            score += 1
        
        # Bonus 4 : Organisation reconnue (+10)
        org = item.get("organization") or {}
        org_name = (org.get("name", "") if isinstance(org, dict) else "").lower()
        recognized_orgs = ['insee', 'ministÃ¨re', 'gouvernement', 'ademe', 'Ã©tat']
        if any(org in org_name for org in recognized_orgs):
            score += 10
        
        return score
    
    async def _fetch_dataset_preview(self, url: str, format_type: str) -> Optional[Dict]:
        """
        TÃ©lÃ©charge un aperÃ§u des 5 premiÃ¨res lignes d'un dataset avec le nombre total de lignes
        
        Args:
            url: URL du dataset
            format_type: Type de fichier (csv, xlsx, xls)
        
        Returns:
            Dict avec 'preview' (5 premiÃ¨res lignes) et 'total_rows' ou None si Ã©chec
        """
        try:
            async with httpx.AsyncClient(timeout=15.0) as client:
                response = await client.get(url, follow_redirects=True)
                
                if response.status_code != 200:
                    return None
                
                content = response.content
                
                if format_type == 'csv':
                    # Parser CSV complet pour compter les lignes
                    text = content.decode('utf-8', errors='ignore')
                    lines = text.split('\n')
                    # Compter lignes non-vides (exclure header)
                    total_rows = len([line for line in lines[1:] if line.strip()]) if len(lines) > 1 else 0
                    
                    # AperÃ§u des 5 premiÃ¨res lignes
                    preview_lines = lines[:6]  # Header + 5 lignes
                    reader = csv.DictReader(preview_lines)
                    preview = [dict(row) for row in list(reader)[:5]]
                    
                    return {
                        'preview': self.clean_nan_values(preview),
                        'total_rows': total_rows
                    }
                
                elif format_type in ['xlsx', 'xls']:
                    # Parser Excel complet pour compter
                    df_full = pd.read_excel(io.BytesIO(content))
                    total_rows = len(df_full)
                    
                    # AperÃ§u des 5 premiÃ¨res lignes
                    preview = df_full.head(5).to_dict('records')
                    
                    return {
                        'preview': self.clean_nan_values(preview),
                        'total_rows': total_rows
                    }
                
                return None
                
        except Exception as e:
            print(f"   âš ï¸ Impossible de charger l'aperÃ§u: {e}")
            return None
    
    async def search_datagouv_api(self, query: str) -> List[Dict]:
        """
        Recherche directe de datasets sur data.gouv.fr via leur API avec scoring sÃ©mantique
        
        Args:
            query: Termes de recherche
        
        Returns:
            Liste des 5 datasets les plus pertinents avec metadata
        """
        try:
            # Extraire les mots-clÃ©s pertinents pour l'API
            stop_words = ['analyse', 'marchÃ©', 'Ã©tude', 'du', 'de', 'des', 'le', 'la', 'les', 'un', 'une', 'en', 'sur', 'donnÃ©es', 'data']
            keywords = [word for word in query.lower().split() if word not in stop_words and len(word) > 2]
            search_query = ' '.join(keywords) if keywords else query
            
            print(f"ğŸ‡«ğŸ‡· Recherche data.gouv.fr API: {query}")
            print(f"   â†’ Mots-clÃ©s pour API: {search_query}")
            
            # Encoder la query complÃ¨te pour la similaritÃ© sÃ©mantique
            print(f"   ğŸ§  Encoding sÃ©mantique de la requÃªte...")
            query_embedding = self.embedding_model.encode([query])[0]
            
            async with httpx.AsyncClient(timeout=15.0) as client:
                # API de recherche data.gouv.fr - rÃ©cupÃ©rer plus de rÃ©sultats pour mieux scorer
                response = await client.get(
                    "https://www.data.gouv.fr/api/1/datasets/",
                    params={
                        "q": search_query,
                        "page_size": 50,  # AugmentÃ© pour avoir plus de choix
                        "sort": "-followers"  # Trier par popularitÃ© comme base
                    }
                )
                
                if response.status_code != 200:
                    print(f"âŒ Erreur API data.gouv.fr: {response.status_code}")
                    return []
                
                data = response.json()
                print(f"   â†’ {data.get('total', 0)} rÃ©sultats totaux, analyse sÃ©mantique de {len(data.get('data', []))} datasets")
                
                # Scorer et trier les datasets avec embeddings
                scored_items = []
                for item in data.get("data", []):
                    score = self._calculate_semantic_score(item, query_embedding)
                    scored_items.append((score, item))
                
                # Trier par score dÃ©croissant
                scored_items.sort(key=lambda x: x[0], reverse=True)
                
                # Extraire les 5 meilleurs avec leurs ressources + aperÃ§u
                datasets = []
                for score, item in scored_items[:5]:  # Top 5
                    title = item.get("title", "Dataset")
                    description = item.get("description", "")[:200]
                    org = item.get("organization") or {}
                    organization = org.get("name", "Inconnu") if isinstance(org, dict) else "Inconnu"
                    resources = item.get("resources", [])
                    
                    print(f"   âœ“ [Score: {score:.1f}] {title[:60]}")
                    
                    # Chercher des ressources CSV/Excel
                    for resource in resources:
                        url = resource.get("url", "")
                        format_type = (resource.get("format") or "").lower()
                        resource_title = resource.get("title", "DonnÃ©es")
                        
                        if format_type in ['csv', 'xlsx', 'xls'] or any(ext in url.lower() for ext in ['.csv', '.xlsx', '.xls']):
                            # TÃ©lÃ©charger un aperÃ§u du dataset
                            print(f"      ğŸ“¥ Chargement aperÃ§u...")
                            preview_data = await self._fetch_dataset_preview(url, format_type)
                            
                            dataset_entry = {
                                "title": f"{title} - {resource_title}",
                                "url": url,
                                "type": format_type or self._detect_format_from_url(url),
                                "description": description,
                                "organization": organization,
                                "source": "data.gouv.fr",
                                "relevance_score": float(round(score, 2)),
                                "preview": preview_data.get('preview') if preview_data else None,
                                "preview_columns": list(preview_data['preview'][0].keys()) if preview_data and preview_data.get('preview') and len(preview_data['preview']) > 0 else [],
                                "total_rows": preview_data.get('total_rows', 0) if preview_data else 0
                            }
                            
                            datasets.append(dataset_entry)
                            break  # Prendre la premiÃ¨re ressource CSV/Excel seulement
                
                print(f"âœ… TrouvÃ© {len(datasets)} datasets pertinents sur data.gouv.fr")
                return datasets
                
        except Exception as e:
            print(f"âŒ Erreur recherche data.gouv.fr: {e}")
            return []
    
    def _detect_format_from_url(self, url: str) -> str:
        """DÃ©tecte le format depuis l'URL"""
        url_lower = url.lower()
        if '.csv' in url_lower:
            return 'csv'
        elif '.xlsx' in url_lower:
            return 'xlsx'
        elif '.xls' in url_lower:
            return 'xls'
        return 'unknown'
    
    async def search_insee_api(self, query: str) -> List[Dict]:
        """
        Recherche de sÃ©ries temporelles INSEE via leur API
        
        Args:
            query: Termes de recherche
        
        Returns:
            Liste de datasets INSEE
        """
        try:
            print(f"ğŸ‡«ğŸ‡· Recherche INSEE API: {query}")
            
            # Pour l'instant, retourner vide car l'API INSEE nÃ©cessite setup complexe
            # Les datasets INSEE seront trouvÃ©s via data.gouv.fr qui rÃ©fÃ©rence l'INSEE
            print("ğŸ“Š INSEE: utiliser data.gouv.fr comme proxy")
            return []
                
        except Exception as e:
            print(f"âŒ Erreur recherche INSEE: {e}")
            return []
    
    async def search_insee_api(self, query: str) -> List[Dict]:
        """
        Recherche de sÃ©ries temporelles INSEE via leur API
        
        Args:
            query: Termes de recherche
        
        Returns:
            Liste de datasets INSEE
        """
        try:
            print(f"ğŸ‡«ğŸ‡· Recherche INSEE API: {query}")
            
            # API INSEE nÃ©cessite une clÃ© API, mais on peut chercher dans les sÃ©ries publiÃ©es
            # Pour l'instant, retourner une liste vide (nÃ©cessite authentification)
            # TODO: ImplÃ©menter avec clÃ© API INSEE si disponible
            
            async with httpx.AsyncClient(timeout=15.0) as client:
                # Recherche dans les donnÃ©es locales INSEE (fichiers Excel disponibles)
                # Utiliser l'index des publications INSEE
                response = await client.get(
                    "https://www.insee.fr/fr/statistiques/recherche",
                    params={
                        "q": query,
                        "debut": 0
                    },
                    follow_redirects=True
                )
                
                if response.status_code != 200:
                    print(f"âš ï¸ INSEE API non disponible")
                    return []
                
                # Pour l'instant, retourner vide car l'API INSEE nÃ©cessite setup complexe
                # Les datasets INSEE seront trouvÃ©s via data.gouv.fr qui rÃ©fÃ©rence l'INSEE
                print("ğŸ“Š INSEE: utiliser data.gouv.fr comme proxy")
                return []
                
        except Exception as e:
            print(f"âŒ Erreur recherche INSEE: {e}")
            return []
    
    def is_dataset_url(self, url: str) -> bool:
        """VÃ©rifie si une URL pointe vers un dataset supportÃ© (CSV ou Excel uniquement)"""
        url_lower = url.lower()
        
        # Exclure PDF et XML
        if any(url_lower.endswith(ext) for ext in ['.pdf', '.xml']):
            return False
        
        # Accepter uniquement CSV et Excel
        return any(url_lower.endswith(fmt) for fmt in self.supported_formats)
    
    async def scrape_dataset_links(self, page_url: str) -> List[str]:
        """
        Scrape une page HTML pour extraire les liens vers des datasets (CSV, Excel)
        
        Args:
            page_url: URL de la page Ã  scraper
        
        Returns:
            Liste des URLs de datasets trouvÃ©es
        """
        try:
            # Ignorer les PDF et XML
            if page_url.lower().endswith(('.pdf', '.xml')):
                print(f"â­ï¸ Fichier non HTML ignorÃ©: {page_url}")
                return []
            
            print(f"ğŸ” Scraping de la page: {page_url}")
            
            async with httpx.AsyncClient(timeout=15.0, follow_redirects=True) as client:
                response = await client.get(page_url)
                
                if response.status_code != 200:
                    print(f"âŒ Erreur HTTP {response.status_code}")
                    return []
                
                # Parser le HTML
                soup = BeautifulSoup(response.text, 'lxml')
                
                dataset_urls = []
                
                # SPÃ‰CIFIQUE data.gouv.fr : Chercher les ressources
                if 'data.gouv.fr' in page_url:
                    print("  ğŸ‡«ğŸ‡· DÃ©tection data.gouv.fr - Recherche spÃ©cifique")
                    # Chercher les liens de ressources
                    for resource in soup.find_all(['a', 'div'], class_=lambda x: x and 'resource' in str(x).lower()):
                        if resource.get('href'):
                            href = resource['href']
                            absolute_url = urljoin(page_url, href)
                            if self.is_dataset_url(absolute_url):
                                dataset_urls.append(absolute_url)
                                print(f"  âœ… Ressource data.gouv.fr: {absolute_url}")
                    
                    # Chercher dans data-href et onclick
                    for elem in soup.find_all(attrs={'data-href': True}):
                        href = elem['data-href']
                        absolute_url = urljoin(page_url, href)
                        if self.is_dataset_url(absolute_url):
                            dataset_urls.append(absolute_url)
                            print(f"  âœ… Data-href data.gouv.fr: {absolute_url}")
                
                # Recherche gÃ©nÃ©rique pour tous les sites
                
                # Chercher tous les liens <a>
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    
                    # Construire l'URL absolue
                    absolute_url = urljoin(page_url, href)
                    
                    # VÃ©rifier si c'est un lien vers un dataset
                    if self.is_dataset_url(absolute_url):
                        # VÃ©rifier que ce n'est pas un lien interne de navigation
                        if not any(skip in absolute_url.lower() for skip in ['login', 'signup', 'auth', 'account']):
                            dataset_urls.append(absolute_url)
                            print(f"  âœ… Dataset trouvÃ©: {absolute_url}")
                
                # Chercher dans download links et export buttons
                for elem in soup.find_all(['a', 'button'], class_=lambda x: x and any(
                    term in str(x).lower() for term in ['download', 'export', 'tÃ©lÃ©charger', 'data']
                )):
                    if elem.get('href'):
                        absolute_url = urljoin(page_url, elem['href'])
                        if self.is_dataset_url(absolute_url):
                            dataset_urls.append(absolute_url)
                            print(f"  âœ… Dataset trouvÃ© (bouton): {absolute_url}")
                    elif elem.get('data-url'):
                        absolute_url = urljoin(page_url, elem['data-url'])
                        if self.is_dataset_url(absolute_url):
                            dataset_urls.append(absolute_url)
                            print(f"  âœ… Dataset trouvÃ© (data-url): {absolute_url}")
                
                # DÃ©dupliquer
                dataset_urls = list(set(dataset_urls))
                
                print(f"ğŸ“Š Total datasets extraits: {len(dataset_urls)}")
                return dataset_urls
                
        except Exception as e:
            print(f"âŒ Erreur lors du scraping de {page_url}: {e}")
            return []
    
    async def download_and_parse(self, url: str) -> Optional[Dict]:
        """
        TÃ©lÃ©charge et parse un dataset depuis une URL
        
        Args:
            url: URL du dataset
        
        Returns:
            Dict avec structure:
            {
                "format": "csv" | "excel",
                "rows": [{"col1": "val1", "col2": "val2"}, ...],
                "preview": [liste des 5 premiÃ¨res lignes],
                "columns": ["col1", "col2", ...],
                "total_rows": int
            }
        """
        
        if not self.is_dataset_url(url):
            print(f"âš ï¸ URL non supportÃ©e: {url}")
            return None
        
        try:
            print(f"ğŸ“¥ TÃ©lÃ©chargement du dataset: {url}")
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(url, follow_redirects=True)
                
                if response.status_code != 200:
                    print(f"âŒ Erreur HTTP {response.status_code} pour {url}")
                    return None
                
                # VÃ©rifier la taille
                content_length = len(response.content)
                if content_length > self.max_file_size:
                    print(f"âš ï¸ Fichier trop volumineux: {content_length / 1024 / 1024:.2f} MB")
                    return None
                
                # Parser selon le format
                if url.lower().endswith('.csv'):
                    return await self._parse_csv(response.content, url)
                elif url.lower().endswith(('.xlsx', '.xls')):
                    return await self._parse_excel(response.content, url)
                
        except httpx.TimeoutException:
            print(f"â±ï¸ Timeout lors du tÃ©lÃ©chargement de {url}")
            return None
        except Exception as e:
            print(f"âŒ Erreur lors du parsing de {url}: {e}")
            return None
    
    async def _parse_csv(self, content: bytes, url: str) -> Dict:
        """Parse un fichier CSV"""
        try:
            # Essayer diffÃ©rents encodages
            for encoding in ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']:
                try:
                    text_content = content.decode(encoding)
                    break
                except UnicodeDecodeError:
                    continue
            else:
                print(f"âŒ Impossible de dÃ©coder le CSV avec les encodages communs")
                return None
            
            # DÃ©tecter le dÃ©limiteur
            sniffer = csv.Sniffer()
            sample = text_content[:1024]
            try:
                dialect = sniffer.sniff(sample)
                delimiter = dialect.delimiter
            except:
                delimiter = ','  # Par dÃ©faut
            
            # Parser le CSV
            reader = csv.DictReader(io.StringIO(text_content), delimiter=delimiter)
            rows = list(reader)
            
            if not rows:
                print(f"âš ï¸ CSV vide: {url}")
                return None
            
            # Remplacer les chaÃ®nes vides et 'nan' par None
            for row in rows:
                for key in row:
                    if row[key] == '' or row[key] == 'nan' or row[key] == 'NaN':
                        row[key] = None
            
            columns = list(rows[0].keys())
            preview = rows[:5]  # 5 premiÃ¨res lignes
            
            print(f"âœ… CSV parsÃ©: {len(rows)} lignes, {len(columns)} colonnes")
            
            # Construire le rÃ©sultat
            result = {
                "format": "csv",
                "url": url,
                "rows": rows,
                "preview": preview,
                "columns": columns,
                "total_rows": len(rows)
            }
            
            # NETTOYAGE FINAL : Ã©liminer rÃ©cursivement tous les NaN restants
            result = self.clean_nan_values(result)
            
            # VALIDATION : vÃ©rifier qualitÃ© du dataset
            if not self.validate_dataset_quality(result):
                return None
            
            return result
            
        except Exception as e:
            print(f"âŒ Erreur parsing CSV: {e}")
            return None
    
    async def _parse_excel(self, content: bytes, url: str) -> Dict:
        """Parse un fichier Excel"""
        try:
            # Utiliser pandas pour parser Excel
            df = pd.read_excel(io.BytesIO(content), engine='openpyxl')
            
            if df.empty:
                print(f"âš ï¸ Excel vide: {url}")
                return None
            
            # Remplacer TOUS les NaN/inf par None (crucial pour JSON)
            df = df.replace([float('inf'), float('-inf')], None)
            df = df.fillna(value=None)
            
            # Convertir en dictionnaire
            rows = df.to_dict('records')
            columns = [str(col) for col in df.columns.tolist()]  # Forcer en string
            preview = rows[:5]  # 5 premiÃ¨res lignes
            
            print(f"âœ… Excel parsÃ©: {len(rows)} lignes, {len(columns)} colonnes")
            
            # Construire le rÃ©sultat
            result = {
                "format": "excel",
                "url": url,
                "rows": rows,
                "preview": preview,
                "columns": columns,
                "total_rows": len(rows)
            }
            
            # NETTOYAGE FINAL : Ã©liminer rÃ©cursivement tous les NaN restants
            result = self.clean_nan_values(result)
            
            # VALIDATION : vÃ©rifier qualitÃ© du dataset
            if not self.validate_dataset_quality(result):
                return None
            
            return result
            
        except Exception as e:
            print(f"âŒ Erreur parsing Excel: {e}")
            return None
    
    def format_preview_for_display(self, dataset: Dict) -> str:
        """Formate les 5 premiÃ¨res lignes pour affichage"""
        if not dataset or not dataset.get("preview"):
            return "Aucune donnÃ©e disponible"
        
        preview = dataset["preview"]
        columns = dataset["columns"]
        
        # CrÃ©er un tableau texte
        output = f"ğŸ“Š Dataset ({dataset['total_rows']} lignes, {len(columns)} colonnes)\n\n"
        output += "Colonnes: " + ", ".join(columns) + "\n\n"
        output += "AperÃ§u (5 premiÃ¨res lignes):\n"
        
        for i, row in enumerate(preview, 1):
            output += f"Ligne {i}: " + " | ".join([f"{k}={v}" for k, v in row.items()]) + "\n"
        
        return output
