"""
Service LLM pour g√©n√©rer les analyses de march√©
Utilise OpenAI GPT pour orchestrer l'analyse avec d√©tection d'intention
"""

from typing import List, Dict, Optional, Tuple
import json
from openai import OpenAI
import os
from dotenv import load_dotenv
import asyncio

# Charger les variables d'environnement depuis le fichier .env du backend
backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
env_path = os.path.join(backend_dir, '.env')
load_dotenv(env_path)


class LLMService:
    """Service pour g√©n√©rer des analyses avec un LLM"""
    
    def __init__(self, api_key: str = None, tavily_service=None, data_service=None):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.model = "gpt-4o-mini"  # Mod√®le plus r√©cent et √©conomique
        self.tavily_service = tavily_service
        self.data_service = data_service
        
        # Configurer OpenAI client
        if self.api_key:
            self.client = OpenAI(api_key=self.api_key)
        else:
            self.client = None
    
    def is_configured(self) -> bool:
        """V√©rifie si le service est configur√©"""
        configured = self.api_key is not None and self.api_key != "your_openai_api_key_here" and self.client is not None
        print(f"üîë LLM configur√©: {configured} (api_key: {bool(self.api_key)}, client: {bool(self.client)})")
        return configured
    
    async def detect_market_analysis_intent(self, user_input: str) -> Tuple[bool, str]:
        """
        D√©tecte si l'entr√©e utilisateur correspond √† une demande d'analyse de march√©
        
        Args:
            user_input: La requ√™te de l'utilisateur
        
        Returns:
            Tuple[bool, str]: (est_analyse_marche, explication)
        """
        
        if not self.is_configured():
            # En mode mock, accepter toute demande
            return True, "Mode simulation activ√©"
        
        try:
            detection_prompt = """Tu es un classificateur d'intentions. Ton r√¥le est de d√©terminer si une demande utilisateur concerne une analyse de march√©.

Une analyse de march√© inclut :
- √âtude d'un secteur, d'une industrie ou d'un march√© sp√©cifique
- Analyse de la concurrence
- Tendances du march√©
- Opportunit√©s commerciales
- Donn√©es sur les consommateurs, produits ou services
- Pr√©visions √©conomiques d'un secteur

Une analyse de march√© N'inclut PAS :
- Questions g√©n√©rales non li√©es au business
- Demandes personnelles
- Conversations informelles
- Questions techniques sans contexte march√©

R√©ponds UNIQUEMENT par un JSON avec ce format exact :
{"is_market_analysis": true/false, "explanation": "explication courte"}

Ne r√©ponds QUE par le JSON, rien d'autre."""

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": detection_prompt},
                    {"role": "user", "content": f"Demande √† classifier : {user_input}"}
                ],
                max_tokens=150,
                temperature=0.3
            )
            
            result_text = response.choices[0].message.content.strip()
            result = json.loads(result_text)
            
            return result.get("is_market_analysis", False), result.get("explanation", "")
            
        except Exception as e:
            print(f"Erreur lors de la d√©tection d'intention: {e}")
            # En cas d'erreur, accepter la demande par d√©faut
            return True, "D√©tection d'intention non disponible"
    
    async def enrich_market_query(self, user_input: str) -> str:
        """
        Enrichit et clarifie le prompt utilisateur pour optimiser la recherche Tavily
        
        Args:
            user_input: Le prompt original de l'utilisateur
        
        Returns:
            str: Prompt enrichi et structur√©
        """
        
        if not self.is_configured():
            return user_input
        
        try:
            enrichment_prompt = """Tu es un expert en formulation de requ√™tes d'analyse de march√©.

Ton r√¥le : transformer un prompt utilisateur en une requ√™te structur√©e et optimis√©e pour un moteur de recherche.

Instructions :
- Ajoute du contexte pertinent implicite
- Pr√©cise le p√©rim√®tre g√©ographique si non mentionn√© (France/Europe par d√©faut)
- Structure la requ√™te avec des mots-cl√©s pertinents
- Ajoute des aspects cl√©s d'analyse (tendances, acteurs, donn√©es chiffr√©es)
- Reste concis (max 2-3 phrases)
- Utilise un langage adapt√© √† la recherche web professionnelle

R√©ponds UNIQUEMENT par la requ√™te enrichie, sans pr√©ambule ni explication."""

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": enrichment_prompt},
                    {"role": "user", "content": f"Prompt √† enrichir : {user_input}"}
                ],
                max_tokens=200,
                temperature=0.5
            )
            
            enriched_query = response.choices[0].message.content.strip()
            print(f"üìù Prompt enrichi : {enriched_query}")
            return enriched_query
            
        except Exception as e:
            print(f"Erreur lors de l'enrichissement: {e}")
            return user_input
    
    async def _enrich_for_datasets(self, user_query: str) -> str:
        """
        Enrichit sp√©cifiquement pour trouver des datasets pertinents
        Ajoute des termes techniques et statistiques pr√©cis
        """
        if not self.is_configured():
            return user_query
        
        try:
            prompt = f"""Transforme cette requ√™te pour trouver des DATASETS sp√©cifiques (CSV/Excel) sur le sujet.

Requ√™te: "{user_query}"

R√®gles:
- Identifier les termes EXACTS pour des datasets (ex: "immatriculations v√©hicules √©lectriques" plut√¥t que "march√© automobile")
- Ajouter des qualificatifs statistiques: donn√©es, chiffres, statistiques, s√©ries temporelles
- Inclure des termes techniques du domaine
- Pr√©ciser la zone g√©ographique si pertinent (France, r√©gion, etc.)
- R√©pondre UNIQUEMENT par la requ√™te (max 12 mots)

Exemples:
"march√© v√©hicules √©lectriques" ‚Üí "immatriculations v√©hicules √©lectriques hybrides rechargeables France statistiques ventes"
"opportunit√©s IA" ‚Üí "intelligence artificielle investissements d√©ploiement France donn√©es chiffres march√©"
"immobilier Paris" ‚Üí "prix immobilier transactions logements Paris Ile-de-France donn√©es notaires"

Requ√™te pour datasets:"""
            
            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Tu es un expert en recherche de donn√©es statistiques."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=80
            )
            
            enriched = response.choices[0].message.content.strip()
            return enriched if enriched else user_query
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur enrichissement datasets: {e}")
            return user_query
    
    async def format_tavily_response(self, tavily_results: List, user_query: str) -> str:
        """
        Reformule les r√©sultats Tavily en fran√ßais structur√© et professionnel
        
        Args:
            tavily_results: Liste des r√©sultats de recherche Tavily
            user_query: La requ√™te originale de l'utilisateur
        
        Returns:
            str: Analyse format√©e en fran√ßais
        """
        
        if not self.is_configured():
            return self._generate_mock_response(user_query)
        
        try:
            # Extraire le contenu des r√©sultats Tavily (ce sont des objets SearchResult)
            context = "\n\n".join([
                f"Source {i+1}: {result.title}\n{result.snippet}"
                for i, result in enumerate(tavily_results[:5])
            ])
            
            formatting_prompt = """Tu es un analyste de march√© professionnel. 

Ton r√¥le : synth√©tiser des informations brutes de recherche web en une analyse de march√© structur√©e, claire et professionnelle en fran√ßais.

Instructions :
- Utilise un fran√ßais impeccable et professionnel
- Structure l'analyse avec des sections claires (tendances, acteurs, opportunit√©s, etc.)
- Utilise des puces pour les listes
- Mets en gras (**texte**) les points importants
- Cite les chiffres et donn√©es factuelles quand disponibles
- Reste objectif et factuel
- Longueur : 300-500 mots

Format souhait√© :
**Analyse de march√© : [Sujet]**

**Vue d'ensemble**
[Contexte g√©n√©ral]

**Tendances principales**
‚Ä¢ Point 1
‚Ä¢ Point 2

**Acteurs cl√©s**
[Description]

**Opportunit√©s**
‚Ä¢ Opportunit√© 1

**Recommandations**
[Synth√®se]"""

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": formatting_prompt},
                    {"role": "user", "content": f"Requ√™te utilisateur : {user_query}\n\nInformations collect√©es :\n{context}\n\nR√©dige l'analyse de march√©."}
                ],
                max_tokens=1500,
                temperature=0.7
            )
            
            formatted_response = response.choices[0].message.content.strip()
            return formatted_response
            
        except Exception as e:
            print(f"Erreur lors du formatage: {e}")
            return self._generate_mock_response(user_query)
    async def analyze_user_input(self, user_input: str) -> str:
        """
        Analyse l'entr√©e utilisateur avec d√©tection d'intention et enrichissement
        
        Flux :
        1. D√©tecte si c'est une demande d'analyse de march√©
        2. Si non : refuse et demande de reformuler
        3. Si oui : enrichit le prompt, appelle Tavily, reformule la r√©ponse
        
        Args:
            user_input: La requ√™te de l'utilisateur depuis la ChatBox
        
        Returns:
            str: R√©ponse g√©n√©r√©e (analyse ou message d'erreur)
        """
        
        print(f"üîç Analyse de la requ√™te : {user_input}")
        
        # √âtape 1 : D√©tection d'intention
        is_market_analysis, explanation = await self.detect_market_analysis_intent(user_input)
        
        if not is_market_analysis:
            print(f"‚ùå Intention non valide : {explanation}")
            return {"response": f"""**Demande non compatible avec l'analyse de march√©**

Votre demande ne semble pas correspondre √† une analyse de march√©.

**Raison** : {explanation}

**Pour obtenir une analyse de march√©, veuillez reformuler votre demande en pr√©cisant :**
‚Ä¢ Le secteur ou l'industrie √† analyser
‚Ä¢ Le type d'information recherch√© (tendances, concurrence, opportunit√©s)
‚Ä¢ Le p√©rim√®tre g√©ographique si pertinent

**Exemples de requ√™tes valides :**
‚Ä¢ "Analyse du march√© des v√©hicules √©lectriques en Europe"
‚Ä¢ "Tendances du e-commerce en France"
‚Ä¢ "Opportunit√©s dans le secteur de l'intelligence artificielle"
‚Ä¢ "Analyse de la concurrence dans le march√© du luxe"

N'h√©sitez pas √† reformuler votre demande ! üîç""", "sources": [], "datasets": []}
        
        print(f"‚úÖ Intention valide : {explanation}")
        
        # √âtape 2 : Enrichissement du prompt
        enriched_query = await self.enrich_market_query(user_input)
        
        # √âtape 3 : Recherche Tavily pour le contexte textuel uniquement
        if self.tavily_service and self.tavily_service.is_configured():
            print(f"üåê Recherche Tavily avec : {enriched_query}")
            try:
                # Recherche g√©n√©rale pour le contexte
                tavily_general_results = await self.tavily_service.search(
                    query=enriched_query,
                    mode="general",
                    max_results=5
                )
                
                # G√©rer les erreurs potentielles
                if isinstance(tavily_general_results, Exception):
                    print(f"‚ö†Ô∏è Erreur recherche g√©n√©rale: {tavily_general_results}")
                    tavily_general_results = []
                
                print(f"üìä Tavily g√©n√©ral: {len(tavily_general_results)} r√©sultats")
                
                # Si aucun r√©sultat, utiliser le LLM direct
                if not tavily_general_results or len(tavily_general_results) == 0:
                    print("‚ö†Ô∏è Aucun r√©sultat Tavily - Basculement vers LLM direct")
                    response = await self._direct_llm_response(user_input)
                    return {"response": response, "sources": []}
                
                # √âtape 4 : Reformulation de la r√©ponse Tavily
                formatted_response = await self.format_tavily_response(tavily_general_results, user_input)
                
                # Pr√©parer les sources pour le frontend
                sources_data = [{
                    "title": result.title,
                    "url": result.url
                } for result in tavily_general_results]
                
                # Retourner avec sources uniquement (pas de datasets pour l'instant)
                return {
                    "response": formatted_response,
                    "sources": sources_data
                }
                
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur Tavily : {e}")
                # Fallback : r√©ponse directe du LLM sans Tavily
                response = await self._direct_llm_response(user_input)
                return {"response": response, "sources": []}
        else:
            print("‚ö†Ô∏è Tavily non configur√© - R√©ponse LLM directe")
            # Fallback : r√©ponse directe du LLM sans Tavily
            response = await self._direct_llm_response(user_input)
            return {"response": response, "sources": []}
    
    def _is_dataset_url(self, url: str) -> bool:
        """V√©rifie si une URL pointe vers un dataset"""
        url_lower = url.lower()
        return any(url_lower.endswith(ext) for ext in ['.csv', '.xlsx', '.xls'])
    
    def _detect_file_type(self, url: str) -> str:
        """D√©tecte le type de fichier depuis l'URL"""
        url_lower = url.lower()
        if url_lower.endswith('.csv'):
            return 'csv'
        elif url_lower.endswith(('.xlsx', '.xls')):
            return 'excel'
        return 'unknown'
    
    async def _validate_datasets_relevance(self, datasets: List[Dict], user_query: str) -> List[Dict]:
        """
        Valide la pertinence des datasets trouv√©s par rapport √† la requ√™te utilisateur
        
        Args:
            datasets: Liste des datasets trouv√©s avec title et url
            user_query: La requ√™te originale de l'utilisateur
        
        Returns:
            Liste filtr√©e des datasets pertinents uniquement
        """
        
        if not self.is_configured():
            # En mode non configur√©, retourner tous les datasets
            return datasets
        
        try:
            # Pr√©parer la liste des datasets pour le LLM
            datasets_info = "\n".join([
                f"{i+1}. Titre: {ds['title']}\n   URL: {ds['url']}"
                for i, ds in enumerate(datasets)
            ])
            
            validation_prompt = f"""Tu es un expert en √©valuation de pertinence de donn√©es pour l'analyse de march√©.

Requ√™te utilisateur: "{user_query}"

Datasets trouv√©s:
{datasets_info}

Ton r√¥le: √âvaluer si chaque dataset est DIRECTEMENT PERTINENT pour r√©pondre √† la requ√™te.

Crit√®res de pertinence STRICTS:
- Le dataset doit traiter SP√âCIFIQUEMENT du sujet demand√© (pas seulement un sujet connexe)
- V√©rifier la zone g√©ographique: un dataset r√©gional (ex: Guadeloupe) n'est PAS pertinent pour une analyse France enti√®re
- V√©rifier la granularit√©: "transport en g√©n√©ral" n'est PAS pertinent pour "v√©hicules √©lectriques"
- Le titre/colonnes doivent clairement indiquer des donn√©es sur le sujet exact

REJETER si:
- Le dataset est trop g√©n√©rique (ex: "transport" pour "v√©hicules √©lectriques")
- La zone g√©ographique ne correspond pas (r√©gion vs national)
- Le sujet est connexe mais pas le m√™me (ex: "automobiles en g√©n√©ral" vs "v√©hicules √©lectriques")

ACCEPTER seulement si:
- Le sujet exact est trait√© (pas juste le domaine g√©n√©ral)
- La zone g√©ographique correspond
- Les donn√©es sont exploitables et sp√©cifiques

R√©ponds UNIQUEMENT par un JSON avec ce format exact:
{{
  "relevant_indices": [1, 3],  // Indices des datasets pertinents (commence √† 1)
  "explanation": "Dataset X: score 8/10 car [raison pr√©cise]. Dataset Y: score 3/10 car [raison du rejet]."
}}

Ne r√©ponds QUE par le JSON, rien d'autre."""

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": validation_prompt}
                ],
                max_tokens=300,
                temperature=0.3
            )
            
            result_text = response.choices[0].message.content.strip()
            result = json.loads(result_text)
            
            relevant_indices = result.get("relevant_indices", [])
            explanation = result.get("explanation", "")
            
            print(f"üîç Validation LLM: {explanation}")
            
            # Filtrer les datasets selon les indices pertinents (convertir de 1-indexed √† 0-indexed)
            relevant_datasets = [datasets[i-1] for i in relevant_indices if 0 < i <= len(datasets)]
            
            return relevant_datasets
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur validation pertinence: {e}")
            # En cas d'erreur, retourner tous les datasets pour ne pas bloquer
            return datasets
    
    async def _direct_llm_response(self, user_input: str) -> str:
        """
        G√©n√®re une r√©ponse directe du LLM sans recherche Tavily (fallback)
        """
        
        if not self.is_configured():
            return self._generate_mock_response(user_input)
        
        try:
            system_prompt = """Tu es un outil d'analyse de march√© expert. 
Tu aides les utilisateurs √† analyser les march√©s, les tendances, les concurrents et les opportunit√©s d'affaires.
R√©ponds de mani√®re structur√©e, professionnelle et actionnable en fran√ßais.
Utilise des puces, du gras (**texte**) et structure ton analyse clairement."""
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_input}
                ],
                max_tokens=1000,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"Erreur lors de l'appel √† OpenAI: {e}")
            return self._generate_mock_response(user_input)
    
    async def validate_dataset_content(self, dataset_keywords: str, user_query: str) -> bool:
        """
        Valide la pertinence d'un dataset en analysant ses colonnes et contenu
        
        Args:
            dataset_keywords: String contenant colonnes et premi√®res valeurs du dataset
            user_query: Requ√™te utilisateur originale
        
        Returns:
            True si le dataset est pertinent, False sinon
        """
        if not self.is_configured() or not dataset_keywords:
            return True  # Fallback: accepter si pas de LLM
        
        try:
            prompt = f"""Analyse si ce dataset est pertinent pour la requ√™te utilisateur.

Requ√™te: "{user_query}"

Contenu du dataset:
{dataset_keywords}

Le dataset est-il DIRECTEMENT pertinent pour r√©pondre √† la requ√™te ?

Crit√®res STRICTS:
- Les colonnes doivent traiter du sujet exact (pas juste un domaine connexe)
- Rejeter si trop g√©n√©rique (ex: "transport" vs "v√©hicules √©lectriques")
- Rejeter si zone g√©ographique incorrecte (ex: Guadeloupe vs France)

R√©ponds UNIQUEMENT par "OUI" ou "NON" (un seul mot)."""

            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Tu es un expert en √©valuation de pertinence de donn√©es."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=10
            )
            
            answer = response.choices[0].message.content.strip().upper()
            is_relevant = "OUI" in answer
            
            print(f"  üîç Validation contenu: {'‚úÖ Pertinent' if is_relevant else '‚ùå Non pertinent'}")
            return is_relevant
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur validation contenu: {e}")
            return True  # En cas d'erreur, accepter pour ne pas bloquer
    
    def _generate_mock_response(self, user_input: str) -> str:
        """G√©n√®re une r√©ponse mock pour les tests"""
        return f"""**Analyse de march√© pour: {user_input}**

Voici une analyse pr√©liminaire bas√©e sur votre demande:

**Tendances actuelles:**
- Le march√© montre des signes de croissance continue
- L'innovation technologique reste un facteur cl√©
- Les consommateurs privil√©gient la durabilit√©

**Recommandations:**
- Surveiller les √©volutions r√©glementaires
- Investir dans les technologies √©mergentes
- D√©velopper une strat√©gie de diff√©renciation

*Note: Cette analyse est g√©n√©r√©e en mode simulation. Configurez une cl√© API OpenAI pour obtenir des analyses plus d√©taill√©es.*"""
    
    async def generate_analysis(
        self,
        query: str,
        web_context: str = "",
        sources: List = None
    ) -> Dict:
        
        """
        G√©n√®re une analyse de march√© compl√®te
        
        Args:
            query: La requ√™te utilisateur
            web_context: Contexte des recherches web
            sources: Liste des sources utilis√©es
        
        Returns:
            Dict contenant qualitative et quantitative
        """
        
        # Si pas configur√©, utiliser des donn√©es mock
        if not self.is_configured():
            return self._generate_mock_analysis(query, sources)
        
        # TODO: Impl√©menter l'appel r√©el au LLM avec LangChain
        # Pour l'instant, retourner des donn√©es mock
        return self._generate_mock_analysis(query, sources)
    
    def _generate_mock_analysis(self, query: str, sources: List = None) -> Dict:
        """G√©n√®re une analyse mock bas√©e sur la requ√™te"""
        
        # Extraire le sujet principal
        subject = query.lower()
        
        # Cr√©er des sources URLs si disponibles
        source_urls = []
        if sources:
            source_urls = [s.url for s in sources[:5]]
        
        # G√©n√©rer l'analyse qualitative
        qualitative = {
            "title": f"Analyse Qualitative - {query.title()}",
            "sections": [
                {
                    "subtitle": "Vue d'ensemble du march√©",
                    "content": f"Le march√© {subject} conna√Æt une croissance dynamique port√©e par l'innovation technologique et l'√©volution des comportements des consommateurs. Les donn√©es r√©centes montrent une transformation structurelle du secteur avec l'√©mergence de nouveaux acteurs et mod√®les √©conomiques."
                },
                {
                    "subtitle": "Tendances principales",
                    "content": f"‚Ä¢ Digitalisation acc√©l√©r√©e des processus et services\n‚Ä¢ Croissance de la demande pour des solutions durables\n‚Ä¢ Consolidation du march√© avec des fusions-acquisitions\n‚Ä¢ Innovation continue dans les technologies de pointe\n‚Ä¢ Expansion internationale des principaux acteurs"
                },
                {
                    "subtitle": "Acteurs principaux",
                    "content": f"Le march√© {subject} est domin√© par plusieurs acteurs majeurs qui investissent massivement dans l'innovation et l'expansion. Les leaders du march√© b√©n√©ficient d'√©conomies d'√©chelle importantes tandis que de nouveaux entrants apportent disruption et innovation."
                },
                {
                    "subtitle": "Opportunit√©s",
                    "content": f"‚Ä¢ Forte demande dans les segments premium\n‚Ä¢ March√©s √©mergents en pleine croissance\n‚Ä¢ Technologies disruptives cr√©ant de nouvelles niches\n‚Ä¢ Partenariats strat√©giques et √©cosyst√®mes\n‚Ä¢ Services √† valeur ajout√©e et personnalisation"
                },
                {
                    "subtitle": "D√©fis et risques",
                    "content": f"‚Ä¢ Concurrence intense et guerre des prix\n‚Ä¢ R√©glementations de plus en plus strictes\n‚Ä¢ Volatilit√© des co√ªts des mati√®res premi√®res\n‚Ä¢ Cybers√©curit√© et protection des donn√©es\n‚Ä¢ Changements rapides des pr√©f√©rences consommateurs"
                }
            ],
            "recommendation": f"Le march√© {subject} offre des opportunit√©s strat√©giques significatives. Nous recommandons une approche focalis√©e sur l'innovation, la diff√©renciation et l'expansion g√©ographique cibl√©e pour maximiser la croissance.",
            "sources": source_urls
        }
        
        # G√©n√©rer les donn√©es quantitatives
        quantitative = {
            "marketSize": {
                "labels": ["2021", "2022", "2023", "2024", "2025 (proj.)"],
                "data": [100, 125, 156, 195, 244],
                "unit": "milliards ‚Ç¨",
                "sources": source_urls[:2] if source_urls else []
            },
            "marketShare": {
                "labels": ["Leader A", "Leader B", "Leader C", "Leader D", "Autres"],
                "data": [25, 18, 15, 12, 30],
                "colors": ["#0055B8", "#00A9E0", "#7AC143", "#FDB913", "#95A5A6"],
                "sources": source_urls[2:4] if len(source_urls) > 2 else []
            },
            "regionalGrowth": {
                "labels": ["Europe", "Am√©rique du Nord", "Asie-Pacifique", "Am√©rique Latine", "MEA"],
                "data": [35, 28, 25, 8, 4],
                "growth": [12.5, 15.8, 22.3, 18.5, 14.2],
                "sources": source_urls[4:6] if len(source_urls) > 4 else []
            },
            "priceEvolution": {
                "labels": ["2020", "2021", "2022", "2023", "2024"],
                "avgPrice": [100, 105, 108, 110, 112],
                "marketValue": [85, 100, 125, 156, 195],
                "sources": source_urls[-2:] if len(source_urls) > 1 else []
            }
        }
        
        return {
            "qualitative": qualitative,
            "quantitative": quantitative
        }
