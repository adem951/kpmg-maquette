"""
Service LLM pour gÃ©nÃ©rer les analyses de marchÃ©
Utilise OpenAI GPT pour orchestrer l'analyse avec dÃ©tection d'intention
"""

from typing import List, Dict, Optional, Tuple
import json
from openai import OpenAI
import os
from dotenv import load_dotenv
import asyncio

# Charger les variables d'environnement depuis le fichier .env du backend
backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
env_path = os.path.join(backend_dir, '.env')
load_dotenv(env_path)


class LLMService:
    """Service pour gÃ©nÃ©rer des analyses avec un LLM"""
    
    def __init__(self, api_key: str = None, tavily_service=None):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.model = "gpt-4o-mini"  # ModÃ¨le plus rÃ©cent et Ã©conomique
        self.tavily_service = tavily_service
        
        # Configurer OpenAI client
        if self.api_key:
            self.client = OpenAI(api_key=self.api_key)
        else:
            self.client = None
    
    def is_configured(self) -> bool:
        """VÃ©rifie si le service est configurÃ©"""
        configured = self.api_key is not None and self.api_key != "your_openai_api_key_here" and self.client is not None
        print(f"ğŸ”‘ LLM configurÃ©: {configured} (api_key: {bool(self.api_key)}, client: {bool(self.client)})")
        return configured
    
    async def detect_market_analysis_intent(self, user_input: str) -> Tuple[bool, str]:
        """
        DÃ©tecte si l'entrÃ©e utilisateur correspond Ã  une demande d'analyse de marchÃ©
        
        Args:
            user_input: La requÃªte de l'utilisateur
        
        Returns:
            Tuple[bool, str]: (est_analyse_marche, explication)
        """
        
        if not self.is_configured():
            # En mode mock, accepter toute demande
            return True, "Mode simulation activÃ©"
        
        try:
            detection_prompt = """Tu es un classificateur d'intentions. Ton rÃ´le est de dÃ©terminer si une demande utilisateur concerne une analyse de marchÃ©.

Une analyse de marchÃ© inclut :
- Ã‰tude d'un secteur, d'une industrie ou d'un marchÃ© spÃ©cifique
- Analyse de la concurrence
- Tendances du marchÃ©
- OpportunitÃ©s commerciales
- DonnÃ©es sur les consommateurs, produits ou services
- PrÃ©visions Ã©conomiques d'un secteur

Une analyse de marchÃ© N'inclut PAS :
- Questions gÃ©nÃ©rales non liÃ©es au business
- Demandes personnelles
- Conversations informelles
- Questions techniques sans contexte marchÃ©

RÃ©ponds UNIQUEMENT par un JSON avec ce format exact :
{"is_market_analysis": true/false, "explanation": "explication courte"}

Ne rÃ©ponds QUE par le JSON, rien d'autre."""

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": detection_prompt},
                    {"role": "user", "content": f"Demande Ã  classifier : {user_input}"}
                ],
                max_tokens=150,
                temperature=0.3
            )
            
            result_text = response.choices[0].message.content.strip()
            result = json.loads(result_text)
            
            return result.get("is_market_analysis", False), result.get("explanation", "")
            
        except Exception as e:
            print(f"Erreur lors de la dÃ©tection d'intention: {e}")
            # En cas d'erreur, accepter la demande par dÃ©faut
            return True, "DÃ©tection d'intention non disponible"
    
    async def enrich_market_query(self, user_input: str) -> str:
        """
        Enrichit et clarifie le prompt utilisateur pour optimiser la recherche Tavily
        
        Args:
            user_input: Le prompt original de l'utilisateur
        
        Returns:
            str: Prompt enrichi et structurÃ©
        """
        
        if not self.is_configured():
            return user_input
        
        try:
            enrichment_prompt = """Tu es un expert en formulation de requÃªtes d'analyse de marchÃ©.

Ton rÃ´le : transformer un prompt utilisateur en une requÃªte structurÃ©e et optimisÃ©e pour un moteur de recherche.

Instructions :
- Ajoute du contexte pertinent implicite
- PrÃ©cise le pÃ©rimÃ¨tre gÃ©ographique si non mentionnÃ© (France/Europe par dÃ©faut)
- Structure la requÃªte avec des mots-clÃ©s pertinents
- Ajoute des aspects clÃ©s d'analyse (tendances, acteurs, donnÃ©es chiffrÃ©es)
- Reste concis (max 2-3 phrases)
- Utilise un langage adaptÃ© Ã  la recherche web professionnelle

RÃ©ponds UNIQUEMENT par la requÃªte enrichie, sans prÃ©ambule ni explication."""

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": enrichment_prompt},
                    {"role": "user", "content": f"Prompt Ã  enrichir : {user_input}"}
                ],
                max_tokens=200,
                temperature=0.5
            )
            
            enriched_query = response.choices[0].message.content.strip()
            print(f"ğŸ“ Prompt enrichi : {enriched_query}")
            return enriched_query
            
        except Exception as e:
            print(f"Erreur lors de l'enrichissement: {e}")
            return user_input
    
    async def format_tavily_response(self, tavily_results: List, user_query: str) -> str:
        """
        Reformule les rÃ©sultats Tavily en franÃ§ais structurÃ© et professionnel
        
        Args:
            tavily_results: Liste des rÃ©sultats de recherche Tavily
            user_query: La requÃªte originale de l'utilisateur
        
        Returns:
            str: Analyse formatÃ©e en franÃ§ais
        """
        
        if not self.is_configured():
            return self._generate_mock_response(user_query)
        
        try:
            # Extraire le contenu des rÃ©sultats Tavily (ce sont des objets SearchResult)
            context = "\n\n".join([
                f"Source {i+1}: {result.title}\n{result.snippet}"
                for i, result in enumerate(tavily_results[:5])
            ])
            
            formatting_prompt = """Tu es un analyste de marchÃ© professionnel. 

Ton rÃ´le : synthÃ©tiser des informations brutes de recherche web en une analyse de marchÃ© structurÃ©e, claire et professionnelle en franÃ§ais.

Instructions :
- Utilise un franÃ§ais impeccable et professionnel
- Structure l'analyse avec des sections claires (tendances, acteurs, opportunitÃ©s, etc.)
- Utilise des puces pour les listes
- Mets en gras (**texte**) les points importants
- Cite les chiffres et donnÃ©es factuelles quand disponibles
- Reste objectif et factuel
- Longueur : 300-500 mots

Format souhaitÃ© :
**Analyse de marchÃ© : [Sujet]**

**Vue d'ensemble**
[Contexte gÃ©nÃ©ral]

**Tendances principales**
â€¢ Point 1
â€¢ Point 2

**Acteurs clÃ©s**
[Description]

**OpportunitÃ©s**
â€¢ OpportunitÃ© 1

**Recommandations**
[SynthÃ¨se]"""

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": formatting_prompt},
                    {"role": "user", "content": f"RequÃªte utilisateur : {user_query}\n\nInformations collectÃ©es :\n{context}\n\nRÃ©dige l'analyse de marchÃ©."}
                ],
                max_tokens=1500,
                temperature=0.7
            )
            
            formatted_response = response.choices[0].message.content.strip()
            return formatted_response
            
        except Exception as e:
            print(f"Erreur lors du formatage: {e}")
            return self._generate_mock_response(user_query)
    async def analyze_user_input(self, user_input: str) -> str:
        """
        Analyse l'entrÃ©e utilisateur avec dÃ©tection d'intention et enrichissement
        
        Flux :
        1. DÃ©tecte si c'est une demande d'analyse de marchÃ©
        2. Si non : refuse et demande de reformuler
        3. Si oui : enrichit le prompt, appelle Tavily, reformule la rÃ©ponse
        
        Args:
            user_input: La requÃªte de l'utilisateur depuis la ChatBox
        
        Returns:
            str: RÃ©ponse gÃ©nÃ©rÃ©e (analyse ou message d'erreur)
        """
        
        print(f"ğŸ” Analyse de la requÃªte : {user_input}")
        
        # Ã‰tape 1 : DÃ©tection d'intention
        is_market_analysis, explanation = await self.detect_market_analysis_intent(user_input)
        
        if not is_market_analysis:
            print(f"âŒ Intention non valide : {explanation}")
            return {"response": f"""**Demande non compatible avec l'analyse de marchÃ©**

Votre demande ne semble pas correspondre Ã  une analyse de marchÃ©.

**Raison** : {explanation}

**Pour obtenir une analyse de marchÃ©, veuillez reformuler votre demande en prÃ©cisant :**
â€¢ Le secteur ou l'industrie Ã  analyser
â€¢ Le type d'information recherchÃ© (tendances, concurrence, opportunitÃ©s)
â€¢ Le pÃ©rimÃ¨tre gÃ©ographique si pertinent

**Exemples de requÃªtes valides :**
â€¢ "Analyse du marchÃ© des vÃ©hicules Ã©lectriques en Europe"
â€¢ "Tendances du e-commerce en France"
â€¢ "OpportunitÃ©s dans le secteur de l'intelligence artificielle"
â€¢ "Analyse de la concurrence dans le marchÃ© du luxe"

N'hÃ©sitez pas Ã  reformuler votre demande ! ğŸ”""", "sources": [], "datasets": []}
        
        print(f"âœ… Intention valide : {explanation}")
        
        # Ã‰tape 2 : Enrichissement du prompt
        enriched_query = await self.enrich_market_query(user_input)
        
        # Ã‰tape 3 : Recherches Tavily en parallÃ¨le (texte + datasets)
        if self.tavily_service and self.tavily_service.is_configured():
            print(f"ğŸŒ Recherche Tavily (parallÃ¨le) avec : {enriched_query}")
            try:
                # Lancer les deux recherches en parallÃ¨le
                tavily_general_task = self.tavily_service.search(
                    query=enriched_query,
                    mode="general",
                    max_results=5
                )
                
                tavily_data_task = self.tavily_service.search(
                    query=enriched_query,
                    mode="data",
                    max_results=5
                )
                
                # Attendre les deux rÃ©sultats en parallÃ¨le
                tavily_general_results, tavily_data_results = await asyncio.gather(
                    tavily_general_task,
                    tavily_data_task,
                    return_exceptions=True
                )
                
                # GÃ©rer les erreurs potentielles
                if isinstance(tavily_general_results, Exception):
                    print(f"âš ï¸ Erreur recherche gÃ©nÃ©rale: {tavily_general_results}")
                    tavily_general_results = []
                
                if isinstance(tavily_data_results, Exception):
                    print(f"âš ï¸ Erreur recherche datasets: {tavily_data_results}")
                    tavily_data_results = []
                
                print(f"ğŸ“Š Tavily gÃ©nÃ©ral: {len(tavily_general_results)} rÃ©sultats")
                print(f"ğŸ“Š Tavily datasets: {len(tavily_data_results)} rÃ©sultats")
                
                # Si aucun rÃ©sultat, utiliser le LLM direct
                if (not tavily_general_results or len(tavily_general_results) == 0) and \
                   (not tavily_data_results or len(tavily_data_results) == 0):
                    print("âš ï¸ Aucun rÃ©sultat Tavily - Basculement vers LLM direct")
                    response = await self._direct_llm_response(user_input)
                    return {"response": response, "sources": [], "datasets": []}
                
                # Ã‰tape 4 : Reformulation de la rÃ©ponse Tavily (utiliser rÃ©sultats gÃ©nÃ©raux)
                formatted_response = await self.format_tavily_response(tavily_general_results, user_input)
                
                # PrÃ©parer les sources pour le frontend
                sources_data = [{
                    "title": result.title,
                    "url": result.url
                } for result in tavily_general_results]
                
                # PrÃ©parer les datasets (URLs vers CSV/Excel trouvÃ©es)
                datasets_data = [{
                    "title": result.title,
                    "url": result.url,
                    "type": self._detect_file_type(result.url)
                } for result in tavily_data_results if self._is_dataset_url(result.url)]
                
                print(f"ğŸ“Š Datasets bruts trouvÃ©s: {len(datasets_data)}")
                
                # Valider la pertinence des datasets avec le LLM
                if len(datasets_data) > 0:
                    datasets_data = await self._validate_datasets_relevance(datasets_data, user_input)
                    print(f"âœ… Datasets pertinents aprÃ¨s validation: {len(datasets_data)}")
                
                # Retourner avec sources et datasets
                return {
                    "response": formatted_response,
                    "sources": sources_data,
                    "datasets": datasets_data
                }
                
            except Exception as e:
                print(f"âš ï¸ Erreur Tavily : {e}")
                # Fallback : rÃ©ponse directe du LLM sans Tavily
                response = await self._direct_llm_response(user_input)
                return {"response": response, "sources": [], "datasets": []}
        else:
            print("âš ï¸ Tavily non configurÃ© - RÃ©ponse LLM directe")
            # Fallback : rÃ©ponse directe du LLM sans Tavily
            response = await self._direct_llm_response(user_input)
            return {"response": response, "sources": [], "datasets": []}
    
    def _is_dataset_url(self, url: str) -> bool:
        """VÃ©rifie si une URL pointe vers un dataset"""
        url_lower = url.lower()
        return any(url_lower.endswith(ext) for ext in ['.csv', '.xlsx', '.xls'])
    
    def _detect_file_type(self, url: str) -> str:
        """DÃ©tecte le type de fichier depuis l'URL"""
        url_lower = url.lower()
        if url_lower.endswith('.csv'):
            return 'csv'
        elif url_lower.endswith(('.xlsx', '.xls')):
            return 'excel'
        return 'unknown'
    
    async def _validate_datasets_relevance(self, datasets: List[Dict], user_query: str) -> List[Dict]:
        """
        Valide la pertinence des datasets trouvÃ©s par rapport Ã  la requÃªte utilisateur
        
        Args:
            datasets: Liste des datasets trouvÃ©s avec title et url
            user_query: La requÃªte originale de l'utilisateur
        
        Returns:
            Liste filtrÃ©e des datasets pertinents uniquement
        """
        
        if not self.is_configured():
            # En mode non configurÃ©, retourner tous les datasets
            return datasets
        
        try:
            # PrÃ©parer la liste des datasets pour le LLM
            datasets_info = "\n".join([
                f"{i+1}. Titre: {ds['title']}\n   URL: {ds['url']}"
                for i, ds in enumerate(datasets)
            ])
            
            validation_prompt = f"""Tu es un expert en Ã©valuation de pertinence de donnÃ©es pour l'analyse de marchÃ©.

RequÃªte utilisateur: "{user_query}"

Datasets trouvÃ©s:
{datasets_info}

Ton rÃ´le: Ã‰valuer si chaque dataset est PERTINENT pour rÃ©pondre Ã  la requÃªte.

CritÃ¨res de pertinence:
- Le dataset doit contenir des donnÃ©es liÃ©es au secteur/marchÃ© mentionnÃ©
- Le dataset doit Ãªtre rÃ©cent ou historiquement pertinent
- Le titre/URL suggÃ¨re des donnÃ©es quantitatives exploitables

RÃ©ponds UNIQUEMENT par un JSON avec ce format exact:
{{
  "relevant_indices": [1, 3],  // Indices des datasets pertinents (commence Ã  1)
  "explanation": "Dataset 1 pertinent car..., Dataset 2 non pertinent car..."
}}

Ne rÃ©ponds QUE par le JSON, rien d'autre."""

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": validation_prompt}
                ],
                max_tokens=300,
                temperature=0.3
            )
            
            result_text = response.choices[0].message.content.strip()
            result = json.loads(result_text)
            
            relevant_indices = result.get("relevant_indices", [])
            explanation = result.get("explanation", "")
            
            print(f"ğŸ” Validation LLM: {explanation}")
            
            # Filtrer les datasets selon les indices pertinents (convertir de 1-indexed Ã  0-indexed)
            relevant_datasets = [datasets[i-1] for i in relevant_indices if 0 < i <= len(datasets)]
            
            return relevant_datasets
            
        except Exception as e:
            print(f"âš ï¸ Erreur validation pertinence: {e}")
            # En cas d'erreur, retourner tous les datasets pour ne pas bloquer
            return datasets
    
    async def _direct_llm_response(self, user_input: str) -> str:
        """
        GÃ©nÃ¨re une rÃ©ponse directe du LLM sans recherche Tavily (fallback)
        """
        
        if not self.is_configured():
            return self._generate_mock_response(user_input)
        
        try:
            system_prompt = """Tu es un outil d'analyse de marchÃ© expert. 
Tu aides les utilisateurs Ã  analyser les marchÃ©s, les tendances, les concurrents et les opportunitÃ©s d'affaires.
RÃ©ponds de maniÃ¨re structurÃ©e, professionnelle et actionnable en franÃ§ais.
Utilise des puces, du gras (**texte**) et structure ton analyse clairement."""
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_input}
                ],
                max_tokens=1000,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"Erreur lors de l'appel Ã  OpenAI: {e}")
            return self._generate_mock_response(user_input)
    
    def _generate_mock_response(self, user_input: str) -> str:
        """GÃ©nÃ¨re une rÃ©ponse mock pour les tests"""
        return f"""**Analyse de marchÃ© pour: {user_input}**

Voici une analyse prÃ©liminaire basÃ©e sur votre demande:

**Tendances actuelles:**
- Le marchÃ© montre des signes de croissance continue
- L'innovation technologique reste un facteur clÃ©
- Les consommateurs privilÃ©gient la durabilitÃ©

**Recommandations:**
- Surveiller les Ã©volutions rÃ©glementaires
- Investir dans les technologies Ã©mergentes
- DÃ©velopper une stratÃ©gie de diffÃ©renciation

*Note: Cette analyse est gÃ©nÃ©rÃ©e en mode simulation. Configurez une clÃ© API OpenAI pour obtenir des analyses plus dÃ©taillÃ©es.*"""
    
    async def generate_analysis(
        self,
        query: str,
        web_context: str = "",
        sources: List = None
    ) -> Dict:
        
        """
        GÃ©nÃ¨re une analyse de marchÃ© complÃ¨te
        
        Args:
            query: La requÃªte utilisateur
            web_context: Contexte des recherches web
            sources: Liste des sources utilisÃ©es
        
        Returns:
            Dict contenant qualitative et quantitative
        """
        
        # Si pas configurÃ©, utiliser des donnÃ©es mock
        if not self.is_configured():
            return self._generate_mock_analysis(query, sources)
        
        # TODO: ImplÃ©menter l'appel rÃ©el au LLM avec LangChain
        # Pour l'instant, retourner des donnÃ©es mock
        return self._generate_mock_analysis(query, sources)
    
    def _generate_mock_analysis(self, query: str, sources: List = None) -> Dict:
        """GÃ©nÃ¨re une analyse mock basÃ©e sur la requÃªte"""
        
        # Extraire le sujet principal
        subject = query.lower()
        
        # CrÃ©er des sources URLs si disponibles
        source_urls = []
        if sources:
            source_urls = [s.url for s in sources[:5]]
        
        # GÃ©nÃ©rer l'analyse qualitative
        qualitative = {
            "title": f"Analyse Qualitative - {query.title()}",
            "sections": [
                {
                    "subtitle": "Vue d'ensemble du marchÃ©",
                    "content": f"Le marchÃ© {subject} connaÃ®t une croissance dynamique portÃ©e par l'innovation technologique et l'Ã©volution des comportements des consommateurs. Les donnÃ©es rÃ©centes montrent une transformation structurelle du secteur avec l'Ã©mergence de nouveaux acteurs et modÃ¨les Ã©conomiques."
                },
                {
                    "subtitle": "Tendances principales",
                    "content": f"â€¢ Digitalisation accÃ©lÃ©rÃ©e des processus et services\nâ€¢ Croissance de la demande pour des solutions durables\nâ€¢ Consolidation du marchÃ© avec des fusions-acquisitions\nâ€¢ Innovation continue dans les technologies de pointe\nâ€¢ Expansion internationale des principaux acteurs"
                },
                {
                    "subtitle": "Acteurs principaux",
                    "content": f"Le marchÃ© {subject} est dominÃ© par plusieurs acteurs majeurs qui investissent massivement dans l'innovation et l'expansion. Les leaders du marchÃ© bÃ©nÃ©ficient d'Ã©conomies d'Ã©chelle importantes tandis que de nouveaux entrants apportent disruption et innovation."
                },
                {
                    "subtitle": "OpportunitÃ©s",
                    "content": f"â€¢ Forte demande dans les segments premium\nâ€¢ MarchÃ©s Ã©mergents en pleine croissance\nâ€¢ Technologies disruptives crÃ©ant de nouvelles niches\nâ€¢ Partenariats stratÃ©giques et Ã©cosystÃ¨mes\nâ€¢ Services Ã  valeur ajoutÃ©e et personnalisation"
                },
                {
                    "subtitle": "DÃ©fis et risques",
                    "content": f"â€¢ Concurrence intense et guerre des prix\nâ€¢ RÃ©glementations de plus en plus strictes\nâ€¢ VolatilitÃ© des coÃ»ts des matiÃ¨res premiÃ¨res\nâ€¢ CybersÃ©curitÃ© et protection des donnÃ©es\nâ€¢ Changements rapides des prÃ©fÃ©rences consommateurs"
                }
            ],
            "recommendation": f"Le marchÃ© {subject} offre des opportunitÃ©s stratÃ©giques significatives. Nous recommandons une approche focalisÃ©e sur l'innovation, la diffÃ©renciation et l'expansion gÃ©ographique ciblÃ©e pour maximiser la croissance.",
            "sources": source_urls
        }
        
        # GÃ©nÃ©rer les donnÃ©es quantitatives
        quantitative = {
            "marketSize": {
                "labels": ["2021", "2022", "2023", "2024", "2025 (proj.)"],
                "data": [100, 125, 156, 195, 244],
                "unit": "milliards â‚¬",
                "sources": source_urls[:2] if source_urls else []
            },
            "marketShare": {
                "labels": ["Leader A", "Leader B", "Leader C", "Leader D", "Autres"],
                "data": [25, 18, 15, 12, 30],
                "colors": ["#0055B8", "#00A9E0", "#7AC143", "#FDB913", "#95A5A6"],
                "sources": source_urls[2:4] if len(source_urls) > 2 else []
            },
            "regionalGrowth": {
                "labels": ["Europe", "AmÃ©rique du Nord", "Asie-Pacifique", "AmÃ©rique Latine", "MEA"],
                "data": [35, 28, 25, 8, 4],
                "growth": [12.5, 15.8, 22.3, 18.5, 14.2],
                "sources": source_urls[4:6] if len(source_urls) > 4 else []
            },
            "priceEvolution": {
                "labels": ["2020", "2021", "2022", "2023", "2024"],
                "avgPrice": [100, 105, 108, 110, 112],
                "marketValue": [85, 100, 125, 156, 195],
                "sources": source_urls[-2:] if len(source_urls) > 1 else []
            }
        }
        
        return {
            "qualitative": qualitative,
            "quantitative": quantitative
        }
